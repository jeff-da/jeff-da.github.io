<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jeff Da</title>

    <meta name="author" content="Jeff Da">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jeff Da
                </p>
                <p>I'm a senior research scientist at <a href="https://scale.com/">Scale AI</a>, where I'm on the Gen AI ML team. I work on language model post-training, alignment, and evaluation.
                </p>
                <p>
                  Previously, I was at Amazon, and before that, I was a Predoctoral Young Investigator working with Antoine Bosselut and Yejin Choi on commonsense reasoning and multimodality.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jefzda@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=cJxpdbEAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/_jeffda?lang=en">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jeff-da">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/headshot.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/headshot.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Research</h2>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- Start of Research Papers -->          
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/gsm1k.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.00332">
                <span class="papertitle">A Careful Examination of Large Language Model Performance on Grade School Arithmetic</span>
              </a>
              <br>
              Hugh Zhang, <strong>Jeff Da</strong>, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue
              <br>
              <em>NeurIPS Spotlight (Datasets and Benchmarks Track)</em>, 2024
              <br>
              <p>We create GSM1k, an uncomtaminated replication of the popular GSM8k dataset. We find that several model families tend to show overfitting on GSM8k, but frontier models show generalizability.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/rm.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2407.13887">
                <span class="papertitle">Learning Goal-Conditioned Representations for Language Reward Models</span>
              </a>
              <br>
              Vaskar Nath, Dylan Slack, <strong>Jeff Da</strong>, Yuntao Ma, Hugh Zhang, Spencer Whitehead, Sean Hendryx
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <p>We find that training reward models using a goal-conditioned reward function improves reasoning + general alignment performance.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/nycc.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://aclanthology.org/2023.acl-long.41.pdf">
                <span class="papertitle">Do Androids Laugh at Electric Sheep? Humor Understanding Benchmarks from the New Yorker Caption Contest</span>
              </a>
              <br>
              Jack Hessel, Ana MarasoviÄ‡, Jena D. Hwang, Lillian Lee, <strong>Jeff Da</strong>, Rowan Zellers, Robert Mankoff, Yejin Choi
              <br>
              <em>ACL Best Paper</em>, 2023
              <br>
              <p>We create a dataset based on the New Yorker Cartoon Caption Contest and find that many frontier models struggle with humor understanding.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/fewshot.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.00297">
                <span class="papertitle">Analyzing Commonsense Emergence in Few-shot Knowledge Models
                </span>
              </a>
              <br>
              Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut
              <br>
              <em>AKBC</em>, 2021
              <br>
              <p>We find that we can use few-shot framing to train commonsense knowledge models.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/emu.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2012.04726">
                <span class="papertitle">Edited Media Understanding: Reasoning About Implications of Manipulated Images</span>
              </a>
              <br>
              Jeff Da, Max Forbes, Rowan Zellers, Anthony Zheng, Jena Hwang, Antoine Bosseult, Yejin Choi
              <br>
              <em>ACL</em>, 2021
              <br>
              <p>We create EMU Frames, a multimodal dataset for evaluating model ability to understand the intent and implications of altered images.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/discourse.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1907.01272"><span class="papertitle">Discourse Understanding and Factual Consistency in Abstractive Summarization</span></a>
              <br>
              Saadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys, Kyle Lo, Asli Celikyilmaz, Yejin Choi
              <br>
              <em>EACL</em>, 2021
              <br>
              <p>We propose Co-opNet, a transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/comet.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2010.05953">
                <span class="papertitle">COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs</span>
              </a>
              <br>
              Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, Yejin Choi
              <br>
              <em>AAAI</em>, 2021
              <br>
              <p>We introduce COMET-ATOMIC 2020, a large-scale commonsense knowledge graph dataset.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/commonsense.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.01157">
                <span class="papertitle">Cracking the Contextual Commonsense Code: Understanding commonsense reasoning capabilities of contextual representations</span>
              </a>
              <br>
              Jeff Da, Jungo Kasai
              <br>
              <em>EMNLP Workshop COIN</em>, 2019
              <br>
              <p>We find that transformers encode commonsense in their embedding layers.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two"><img src='images/paper_images/bigmood.png' width="160"></div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.07713">
                <span class="papertitle">BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge</span>
              </a>
              <br>
              Jeff Da
              <br>
              <em>EMNLP Workshop COIN</em>, 2019
              <br>
              <p>We study the ability to post-train language models via explicit commonsense knowledge references.</p>
            </td>
          </tr>
          <!-- End of Research Papers -->
        </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href="https://github.com/jonbarron/jonbarron_website">Website credit + source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
