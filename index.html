<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jeff Da</title>

    <meta name="author" content="Jeff Da">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <style>
      body {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding: 0;
      }
      .name {
        font-size: 32px;
        font-weight: bold;
        margin: 20px 0;
      }
      .papertitle {
        font-weight: bold;
        color: #333;
        text-decoration: none;
      }
      .papertitle:hover {
        color: #0066cc;
      }
      a {
        color: #0066cc;
        text-decoration: none;
      }
      a:hover {
        text-decoration: underline;
      }
      h2 {
        font-size: 24px;
        margin: 30px 0 20px 0;
      }
      p {
        margin: 10px 0;
      }
      /* Fix for image centering */
      .paper-image-cell {
        padding: 16px;
        width: 20%;
        vertical-align: middle !important;
        text-align: center;
      }
      .paper-content-cell {
        padding: 16px 8px;
        width: 80%;
        vertical-align: middle !important;
      }
      .paper-image {
        width: 160px;
        height: auto;
        display: block;
        margin: 0 auto;
      }
    </style>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jeff Da
                </p>
                <p>I'm currently a research scientist at <a href="https://scale.com/">Scale AI</a>.</p>
                <p>At Scale, I lead research on reinforcement learning, agents, and post-training. Prior to Scale, I was at the <a href="">Allen Institute for AI</a> and University of Washington, where I did research on a variety of topics in reasoning, evals, and multimodality. I worked with <a href="">Yejin Choi</a>, with additional mentorship from <a href="">Antoine Bosselut</a>, <a href="">Maxwell Forbes</a>, and many other amazing collaborators.</p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?user=cJxpdbEAAAAJ&hl=en">[Google Scholar]</a>
                  <a href="https://x.com/_jeffda?lang=en">[Twitter]</a>
                  <a href="https://github.com/jeff-da">[Github]</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/headshot.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/headshot.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Blog</h2>
                <ul>
                  <li><a href="https://www.overleaf.com/read/mprnnszgjrdy#85ac22">Agent Alignment: Implications of Scaling RL</a></li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Research</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- Start of Research Papers -->          
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/agentrlvr.png" class="paper-image" alt="Agent-RLVR">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/2506.11425">
                <span class="papertitle">Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards</span>
              </a>
              <br>
              <strong>Jeff Da</strong>, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, Sean Hendryx
              <br>
              <em>arxiv</em>, 2025
              <br>
              <p>We introduce Agent-RLVR, a novel reinforcement learning technique via teacher-student multi-agent co-evolution. To that end, we train SWE-Agents using RL and verifiable rewards that significantly improve upon the base model.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/gsm1k.png" class="paper-image" alt="GSM1k">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/2405.00332">
                <span class="papertitle">A Careful Examination of Large Language Model Performance on Grade School Arithmetic</span>
              </a>
              <br>
              Hugh Zhang, <strong>Jeff Da</strong>, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue
              <br>
              <em>NeurIPS Spotlight (Datasets and Benchmarks Track)</em>, 2024
              <br>
              <p>We create GSM1k, an uncomtaminated replication of the popular GSM8k dataset. We find that several model families tend to show overfitting on GSM8k, but frontier models show generalizability.</p>
            </td>
          </tr>
          
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/rm.png" class="paper-image" alt="Reward Models">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/pdf/2407.13887">
                <span class="papertitle">Learning Goal-Conditioned Representations for Language Reward Models</span>
              </a>
              <br>
              Vaskar Nath, Dylan Slack, <strong>Jeff Da</strong>, Yuntao Ma, Hugh Zhang, Spencer Whitehead, Sean Hendryx
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <p>We find that training reward models using a goal-conditioned reward function improves reasoning + general alignment performance.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/nycc.png" class="paper-image" alt="New Yorker Caption Contest">
            </td>
            <td class="paper-content-cell">
              <a href="https://aclanthology.org/2023.acl-long.41.pdf">
                <span class="papertitle">Do Androids Laugh at Electric Sheep? Humor Understanding Benchmarks from the New Yorker Caption Contest</span>
              </a>
              <br>
              Jack Hessel, Ana MarasoviÄ‡, Jena D. Hwang, Lillian Lee, <strong>Jeff Da</strong>, Rowan Zellers, Robert Mankoff, Yejin Choi
              <br>
              <em>ACL Best Paper</em>, 2023
              <br>
              <p>We create a dataset based on the New Yorker Cartoon Caption Contest and find that many frontier models struggle with humor understanding.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/fewshot.png" class="paper-image" alt="Few-shot Learning">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/2101.00297">
                <span class="papertitle">Analyzing Commonsense Emergence in Few-shot Knowledge Models
                </span>
              </a>
              <br>
              Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut
              <br>
              <em>AKBC</em>, 2021
              <br>
              <p>We find that we can use few-shot framing to train commonsense knowledge models.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/emu.png" class="paper-image" alt="EMU Frames">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/2012.04726">
                <span class="papertitle">Edited Media Understanding: Reasoning About Implications of Manipulated Images</span>
              </a>
              <br>
              Jeff Da, Max Forbes, Rowan Zellers, Anthony Zheng, Jena Hwang, Antoine Bosseult, Yejin Choi
              <br>
              <em>ACL</em>, 2021
              <br>
              <p>We create EMU Frames, a multimodal dataset for evaluating model ability to understand the intent and implications of altered images.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/discourse.png" class="paper-image" alt="Discourse Understanding">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/pdf/1907.01272"><span class="papertitle">Discourse Understanding and Factual Consistency in Abstractive Summarization</span></a>
              <br>
              Saadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys, Kyle Lo, Asli Celikyilmaz, Yejin Choi
              <br>
              <em>EACL</em>, 2021
              <br>
              <p>We propose Co-opNet, a transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/comet.png" class="paper-image" alt="COMET-ATOMIC">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/2010.05953">
                <span class="papertitle">COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs</span>
              </a>
              <br>
              Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, Yejin Choi
              <br>
              <em>AAAI</em>, 2021
              <br>
              <p>We introduce COMET-ATOMIC 2020, a large-scale commonsense knowledge graph dataset.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/commonsense.png" class="paper-image" alt="Commonsense Reasoning">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/1910.01157">
                <span class="papertitle">Cracking the Contextual Commonsense Code: Understanding commonsense reasoning capabilities of contextual representations</span>
              </a>
              <br>
              Jeff Da, Jungo Kasai
              <br>
              <em>EMNLP Workshop COIN</em>, 2019
              <br>
              <p>We find that transformers encode commonsense in their embedding layers.</p>
            </td>
          </tr>
          <tr>
            <td class="paper-image-cell">
              <img src="images/paper_images/bigmood.png" class="paper-image" alt="BIG MOOD">
            </td>
            <td class="paper-content-cell">
              <a href="https://arxiv.org/abs/1910.07713">
                <span class="papertitle">BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge</span>
              </a>
              <br>
              Jeff Da
              <br>
              <em>EMNLP Workshop COIN</em>, 2019
              <br>
              <p>We study the ability to post-train language models via explicit commonsense knowledge references.</p>
            </td>
          </tr>
          <!-- End of Research Papers -->
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href="https://github.com/jonbarron/jonbarron_website">Website credit + source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>